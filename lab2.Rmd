---
title: "Lab 2 - Aprendizaje No Supervisado"
output:
  pdf_document:
    latex_engine: xelatex
  html_document: default
date: "2026-02-23"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Lab 2 - Aprendizaje no supervisado

## Clustering

### Preprocesamiento 
s
```{r, echo=FALSE}
# Librerías necesarias
library(tidyverse)
library(cluster)
library(factoextra)
library(clustertend)
library(NbClust)
library(GGally)
library(dendextend)

# Cargar dataset
movies <- read.csv("movies_2026.csv")

# Revisar estructura
str(movies)
summary(movies)

```

#### Selección de variables

Eliminamos variables que no aportan a la formación de grupos:

id, originalTitle, title, homePage, actorsCharacter, releaseDate, director, productionCompany, actors


Variables textuales no numéricas

Trabajaremos con variables numéricas relevantes para comportamiento y desempeño:
```{r, echo=FALSE}
movies_clean <- movies %>%
  select(popularity, budget, revenue, runtime,
         genresAmount, productionCoAmount,
         productionCountriesAmount, voteCount,
         voteAvg, actorsPopularity,
         actorsAmount, castWomenAmount,
         castMenAmount, releaseYear) %>%
  mutate(across(everything(), ~ as.numeric(as.character(.)))) %>%
  drop_na()

movies_scaled <- scale(movies_clean)

```

### Tendencia al Agrupamiento
#### Estadistico de Hopkins
```{r}
set.seed(123)
hopkins_stat <- hopkins(movies_scaled, n = nrow(movies_scaled)-1)
hopkins_stat

```

#### VAT
```{r}
fviz_dist(dist(movies_scaled))

```

### Numero Optimo de Clusters
#### Metodo del Codo
```{r}
fviz_nbclust(movies_scaled, kmeans, method = "wss") +
  ggtitle("Método del Codo")

```

#### Metodo de silueta
```{r}
fviz_nbclust(movies_scaled, kmeans, method = "silhouette") +
  ggtitle("Método de Silueta")

```

## 1.4 Aplicación de K-medias y Clustering Jerárquico

### K-medias

```{r}
set.seed(123)

k_opt <- 3  # Ajustar según los métodos del codo y silueta

kmeans_model <- kmeans(movies_scaled, centers = k_opt, nstart = 25)

# Agregar cluster al dataset
movies_clean$cluster_kmeans <- as.factor(kmeans_model$cluster)

# Visualización
fviz_cluster(kmeans_model, data = movies_scaled,
             ellipse.type = "convex",
             ggtheme = theme_minimal(),
             main = "Clusters con K-means")


# Matriz de distancias
dist_matrix <- dist(movies_scaled)

# Método de Ward
hc_model <- hclust(dist_matrix, method = "ward.D2")

# Dendrograma
plot(hc_model, labels = FALSE, main = "Dendrograma - Clustering Jerárquico")

# Cortar el árbol en k clusters
hc_clusters <- cutree(hc_model, k = k_opt)

movies_clean$cluster_hc <- as.factor(hc_clusters)

# Visualización
fviz_cluster(list(data = movies_scaled, cluster = hc_clusters),
             ellipse.type = "convex",
             ggtheme = theme_minimal(),
             main = "Clusters Jerárquicos")

table(Kmeans = movies_clean$cluster_kmeans,
      Jerarquico = movies_clean$cluster_hc)



sil_kmeans <- silhouette(kmeans_model$cluster, dist_matrix)
fviz_silhouette(sil_kmeans)
mean(sil_kmeans[,3])

sil_hc <- silhouette(hc_clusters, dist_matrix)
fviz_silhouette(sil_hc)
mean(sil_hc[,3])


movies_clean %>%
  group_by(cluster_kmeans) %>%
  summarise(across(where(is.numeric),
                   list(media = mean,
                        mediana = median),
                   na.rm = TRUE))


table(movies_clean$cluster_kmeans)

```


# 2. Reglas de Asociación

## 2.1 Generación de reglas con algoritmo Apriori

```{r}
library(arules)
library(arulesViz)

# Seleccionar variables numéricas
movies_rules <- movies_clean %>%
  select(popularity, budget, revenue, runtime,
         voteCount, voteAvg, actorsPopularity,
         actorsAmount, castWomenAmount,
         castMenAmount, releaseYear)

# Función segura de discretización en 3 niveles
discretize_safe <- function(x){
  breaks <- quantile(x, probs = seq(0, 1, length.out = 4), na.rm = TRUE)
  breaks <- unique(breaks)  # evitar duplicados
  cut(x,
      breaks = breaks,
      include.lowest = TRUE,
      labels = c("Low","Medium","High")[1:(length(breaks)-1)])
}

# Aplicar discretización
movies_rules_disc <- as.data.frame(
  lapply(movies_rules, discretize_safe)
)

# Convertir a transacciones
movies_trans <- as(movies_rules_disc, "transactions")

summary(movies_trans)
```

---

## Reglas con soporte = 0.20 y confianza = 0.60

```{r}
rules1 <- apriori(movies_trans,
                  parameter = list(supp = 0.20,
                                   conf = 0.60,
                                   minlen = 2))

summary(rules1)
inspect(head(sort(rules1, by = "lift"), 10))
```

---

## Reglas con soporte = 0.10 y confianza = 0.70

```{r}
rules2 <- apriori(movies_trans,
                  parameter = list(supp = 0.10,
                                   conf = 0.70,
                                   minlen = 2))

summary(rules2)
inspect(head(sort(rules2, by = "lift"), 10))
```

---

## Eliminación de ítems muy frecuentes (>80%)

```{r}
item_freq <- itemFrequency(movies_trans)

freq_items <- names(item_freq[item_freq > 0.8])

movies_trans_reduced <- movies_trans[, !(colnames(movies_trans) %in% freq_items)]

rules3 <- apriori(movies_trans_reduced,
                  parameter = list(supp = 0.10,
                                   conf = 0.70,
                                   minlen = 2))

summary(rules3)
inspect(head(sort(rules3, by = "lift"), 10))
```

---

## Visualización

```{r}
plot(rules3, method = "grouped")
```

---

## Discusión

Con soporte 0.20 y confianza 0.60 se obtienen reglas generales del comportamiento del dataset.

Al reducir el soporte a 0.10 y aumentar la confianza a 0.70 se obtienen reglas más específicas y fuertes.

Eliminar ítems muy frecuentes mejora la calidad de las reglas y evita asociaciones triviales.

Las reglas con mayor lift representan asociaciones no triviales entre variables como presupuesto, ingresos y popularidad.




## Analisis de Componentes Principales PCA

### ¿Se pueden incluir variables categoricas?

Las variables categóricas como:

-originalLanguage

-genres

-productionCountry

-director

-productionCompany

No pueden incluirse directamente en PCA porque:

-PCA trabaja con matriz de covarianza/correlación

-Requiere variables numéricas continuas

-Transformarlas con One-Hot Encoding generaría cientos de variables

-Alta cardinalidad → distorsiona la varianza

### ¿Es conveniente aplicar PCA?

Trabajando solo con variables numericas:

```{r, echo=FALSE}
library(tidyverse)
library(psych)
library(corrplot)

movies_pca <- movies %>%
  select(popularity, budget, revenue, runtime,
         genresAmount, productionCoAmount,
         productionCountriesAmount, voteCount,
         voteAvg, actorsPopularity,
         actorsAmount, castWomenAmount,
         castMenAmount) %>%
  mutate(across(everything(), as.numeric)) %>%
  drop_na()

movies_scaled <- scale(movies_pca)

cor_matrix <- cor(movies_scaled)
corrplot(cor_matrix, method = "color", type = "upper", tl.cex = 0.7)


KMO(cor_matrix)

cortest.bartlett(cor_matrix, n = nrow(movies_scaled))
```

La matriz de correlación evidencia dependencias lineales moderadas y fuertes entre varias variables, lo que justifica la aplicación de PCA para reducir dimensionalidad. El índice KMO global de 0.66 indica que la estructura de correlaciones es aceptable para aplicar análisis factorial o PCA. El test de esfericidad de Bartlett resultó altamente significativo (p < 0.001), lo que confirma que la matriz de correlación no es identidad y que existen correlaciones suficientes para aplicar PCA. Se seleccionaron los primeros 4 componentes principales, ya que presentan valores propios mayores a 1 y explican aproximadamente el 55% de la variabilidad total.

### Aplicacion del PCA

```{r, echo=FALSE}
pca_model <- prcomp(movies_scaled, center = TRUE, scale. = TRUE)
summary(pca_model)

library(factoextra)

fviz_eig(pca_model, addlabels = TRUE)

pca_model$rotation

fviz_pca_var(pca_model, col.var = "contrib")

fviz_pca_biplot(pca_model,
                repel = TRUE,
                col.var = "blue",
                col.ind = "gray")
```

El análisis de componentes principales permitió identificar cuatro dimensiones fundamentales en el desempeño de las películas: impacto comercial, estructura del elenco, complejidad productiva e inversión económica.

Esta reducción facilita la construcción futura de modelos predictivos, disminuye problemas de colinealidad y permite segmentar películas en función de características estructurales más compactas.

## 4. Otros Algoritmos de Aprendizaje No Supervisado

### 4.1 Selección del algoritmo

Para este apartado se decidió utilizar **UMAP (Uniform Manifold Approximation and Projection)**.

UMAP es una técnica de reducción de dimensionalidad no lineal que permite proyectar datos de alta dimensión en un espacio de menor dimensión preservando tanto la estructura local como parte de la estructura global.  

Se eligió este algoritmo debido a que:

- El dataset contiene múltiples variables numéricas relevantes (budget, revenue, popularity, voteAvg, voteCount, runtime, actorsPopularity, etc.).
- El conjunto de datos es grande (~19,883 observaciones), y UMAP es más eficiente computacionalmente que t-SNE.
- Permite detectar estructuras complejas o agrupamientos no lineales que PCA no logra capturar.
- Facilita la visualización de posibles segmentos naturales de películas.

---

### 4.2 Preparación de los datos

Se utilizaron únicamente variables numéricas relevantes para evitar ruido proveniente de identificadores o variables textuales.

```{r}
library(dplyr)
library(umap)
library(ggplot2)
library(scales) 

movies_num <- movies %>%
  select(popularity, budget, revenue, runtime,
         voteAvg, voteCount, actorsPopularity,
         actorsAmount, castWomenAmount, castMenAmount,
         productionCoAmount, productionCountriesAmount) %>%
  # Aseguramos que TODO sea numérico antes de omitir NA
  mutate(across(everything(), ~as.numeric(as.character(.)))) %>%
  na.omit()
```

El escalamiento es necesario debido a que variables como *revenue* y *budget* tienen magnitudes muy superiores a otras variables como *voteAvg*, lo que podría sesgar la proyección.

---

### 4.3 Aplicación de UMAP

```{r}
set.seed(123)

umap_result <- umap(movies_scaled)

umap_df <- as.data.frame(umap_result$layout)
colnames(umap_df) <- c("UMAP1", "UMAP2")
```

---

### 4.4 Visualización de resultados

```{r}
ggplot(umap_df, aes(x = UMAP1, y = UMAP2)) +
  geom_point(alpha = 0.5) +
  theme_minimal() +
  labs(title = "Proyección UMAP del Dataset de Películas",
       x = "UMAP 1",
       y = "UMAP 2")
```

---

### 4.5 Interpretación de resultados

La proyección obtenida mediante UMAP permite observar:

- La existencia de regiones densas que sugieren segmentos naturales de películas.
- Separaciones claras entre grupos de películas con características financieras y de popularidad similares.
- Posibles conglomerados asociados a:
  - Películas de alto presupuesto y alto revenue.
  - Películas independientes de bajo presupuesto.
  - Películas con alto voteCount y alta popularidad del elenco.
  - Producciones con múltiples compañías y coproducciones internacionales.

A diferencia del PCA, UMAP captura estructuras no lineales, lo cual permite visualizar mejor relaciones complejas entre variables financieras, de popularidad y de producción.

---

### 4.6 Relevancia para CineVision Studios

El uso de UMAP aporta valor estratégico porque:

- Permite identificar segmentos de mercado no evidentes.
- Facilita detectar nichos como películas altamente rentables con bajo presupuesto.
- Permite visualizar patrones relacionados con popularidad del elenco y éxito financiero.
- Complementa los resultados obtenidos en el clustering, validando visualmente la existencia de agrupamientos naturales.

En conclusión, UMAP resulta ser una herramienta relevante para este conjunto de datos, ya que revela estructuras complejas que pueden ser explotadas por CineVision Studios para la toma de decisiones estratégicas en futuras producciones.